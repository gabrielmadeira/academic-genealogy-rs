{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse as sp\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "import time\n",
    "from sys import getsizeof\n",
    "\n",
    "%load_ext sql\n",
    "\n",
    "import pandas.io.sql as sqlio\n",
    "import psycopg2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "conn = psycopg2.connect(host=\"localhost\", port=5432, dbname=\"thegoldtree\", user=\"postgres\", password=\"postgres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.validation import check_array, check_X_y, check_is_fitted\n",
    "from sklearn.utils.sparsefuncs import csc_median_axis_0\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "\n",
    "# This is a modified version of NearestCentroid from sklearn lib\n",
    "class NearestCentroid(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, metric='euclidean', shrink_threshold=None):\n",
    "        self.metric = metric\n",
    "        self.shrink_threshold = shrink_threshold\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        if self.metric == 'precomputed':\n",
    "            raise ValueError(\"Precomputed is not supported.\")\n",
    "        # If X is sparse and the metric is \"manhattan\", store it in a csc\n",
    "        # format is easier to calculate the median.\n",
    "        if self.metric == 'manhattan':\n",
    "            X, y = check_X_y(X, y, ['csc'])\n",
    "        else:\n",
    "            X, y = check_X_y(X, y, ['csr', 'csc'])\n",
    "        is_X_sparse = sp.issparse(X)\n",
    "        if is_X_sparse and self.shrink_threshold:\n",
    "            raise ValueError(\"threshold shrinking not supported\"\n",
    "                             \" for sparse input\")\n",
    "        check_classification_targets(y)\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        le = LabelEncoder()\n",
    "        y_ind = le.fit_transform(y)\n",
    "        self.classes_ = classes = le.classes_\n",
    "        n_classes = classes.size\n",
    "        if n_classes < 2:\n",
    "            raise ValueError('The number of classes has to be greater than'\n",
    "                             ' one; got %d class' % (n_classes))\n",
    "\n",
    "        # Mask mapping each class to its members.\n",
    "        self.centroids_ = sp.lil_matrix((n_classes, n_features), dtype=np.float64)\n",
    "        # Number of clusters in each class.\n",
    "        nk = np.zeros(n_classes)\n",
    "\n",
    "        for cur_class in range(n_classes):\n",
    "            center_mask = y_ind == cur_class\n",
    "            nk[cur_class] = np.sum(center_mask)\n",
    "            if is_X_sparse:\n",
    "                center_mask = np.where(center_mask)[0]\n",
    "\n",
    "            # XXX: Update other averaging methods according to the metrics.\n",
    "            if self.metric == \"manhattan\":\n",
    "                # NumPy does not calculate median of sparse matrices.\n",
    "                if not is_X_sparse:\n",
    "                    self.centroids_[cur_class] = np.median(X[center_mask], axis=0)\n",
    "                else:\n",
    "                    self.centroids_[cur_class] = csc_median_axis_0(X[center_mask])\n",
    "            else:\n",
    "                if self.metric != 'euclidean':\n",
    "                    warnings.warn(\"Averaging for metrics other than \"\n",
    "                                  \"euclidean and manhattan not supported. \"\n",
    "                                  \"The average is set to be the mean.\"\n",
    "                                  )\n",
    "                self.centroids_[cur_class] = X[center_mask].mean(axis=0)\n",
    "\n",
    "        if self.shrink_threshold:\n",
    "            dataset_centroid_ = np.mean(X, axis=0)\n",
    "\n",
    "            # m parameter for determining deviation\n",
    "            m = np.sqrt((1. / nk) - (1. / n_samples))\n",
    "            # Calculate deviation using the standard deviation of centroids.\n",
    "            variance = (X - self.centroids_[y_ind]) ** 2\n",
    "            variance = variance.sum(axis=0)\n",
    "            s = np.sqrt(variance / (n_samples - n_classes))\n",
    "            s += np.median(s)  # To deter outliers from affecting the results.\n",
    "            mm = m.reshape(len(m), 1)  # Reshape to allow broadcasting.\n",
    "            ms = mm * s\n",
    "            deviation = ((self.centroids_ - dataset_centroid_) / ms)\n",
    "            # Soft thresholding: if the deviation crosses 0 during shrinking,\n",
    "            # it becomes zero.\n",
    "            signs = np.sign(deviation)\n",
    "            deviation = (np.abs(deviation) - self.shrink_threshold)\n",
    "            np.clip(deviation, 0, None, out=deviation)\n",
    "            deviation *= signs\n",
    "            # Now adjust the centroids using the deviation\n",
    "            msd = ms * deviation\n",
    "            self.centroids_ = dataset_centroid_[np.newaxis, :] + msd\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        check_is_fitted(self, 'centroids_')\n",
    "\n",
    "        X = check_array(X, accept_sparse='csr')\n",
    "        \n",
    "        return np.argsort(pairwise_distances(X, self.centroids_, metric=self.metric))[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table relationship (just title and abstract) in pandas:\n",
      "1229067454\n"
     ]
    }
   ],
   "source": [
    "sql = \"select title, abstract, id_advisor, id, id_author from relationship;\"\n",
    "docs = sqlio.read_sql_query(sql, conn)\n",
    "docs['title'] = docs['title'].apply(lambda x: x if isinstance(x, str) else '')\n",
    "docs['abstract'] = docs['abstract'].apply(lambda x: x if isinstance(x, str) else '')\n",
    "print(\"table relationship (just title and abstract) in pandas:\")\n",
    "print(getsizeof(docs['title'] + ' ' + docs['abstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table researcher in pandas:\n",
      "115337586\n"
     ]
    }
   ],
   "source": [
    "sql = \"select * from researcher;\"\n",
    "researcher = sqlio.read_sql_query(sql, conn)\n",
    "print(\"table researcher in pandas:\")\n",
    "print(getsizeof(researcher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizer:\n",
      "56\n",
      "X_doc_vect_representation:\n",
      "56\n",
      "y_doc_vect_representation:\n",
      "104\n",
      "[580937]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = pickle.load(open('vectorizer', 'rb'))\n",
    "print(\"Vectorizer:\")\n",
    "print(getsizeof(vectorizer))\n",
    "X_doc_vect_representation = pickle.load(open('X_doc_vect_representation', 'rb'))\n",
    "print(\"X_doc_vect_representation:\")\n",
    "print(getsizeof(X_doc_vect_representation))\n",
    "y_doc_vect_representation = pickle.load(open('y_doc_vect_representation', 'rb'))\n",
    "print(\"y_doc_vect_representation:\")\n",
    "print(getsizeof(y_doc_vect_representation))\n",
    "print(y_doc_vect_representation[1][400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y_doc_vect_representation:\n",
      "96\n",
      "train_X_doc_vect_representation:\n",
      "56\n",
      "test_y_doc_vect_representation:\n",
      "104\n",
      "test_X_doc_vect_representation:\n",
      "56\n",
      "fold1_clf:\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "train_y_doc_vect_representation = pickle.load(open('folds/fold1_train_y_doc_vect_representation', 'rb'))\n",
    "print(\"train_y_doc_vect_representation:\")\n",
    "print(getsizeof(train_y_doc_vect_representation))\n",
    "\n",
    "train_X_doc_vect_representation = pickle.load(open('folds/fold1_train_X_doc_vect_representation', 'rb'))\n",
    "print(\"train_X_doc_vect_representation:\")\n",
    "print(getsizeof(train_X_doc_vect_representation))\n",
    "\n",
    "test_y_doc_vect_representation = pickle.load(open('folds/fold1_test_y_doc_vect_representation', 'rb'))\n",
    "print(\"test_y_doc_vect_representation:\")\n",
    "print(getsizeof(test_y_doc_vect_representation))\n",
    "\n",
    "test_X_doc_vect_representation = pickle.load(open('folds/fold1_test_X_doc_vect_representation', 'rb'))\n",
    "print(\"test_X_doc_vect_representation:\")\n",
    "print(getsizeof(test_X_doc_vect_representation))\n",
    "\n",
    "fold1_clf = pickle.load(open('clfs/fold1_clf', 'rb'))\n",
    "print(\"fold1_clf:\")\n",
    "print(getsizeof(fold1_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79653, 883149)\n",
      "clf_all_data:\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "clf_all_data1 = pickle.load(open('clfs/clf_all_data1', 'rb'))\n",
    "print(clf_all_data1.centroids_.shape)\n",
    "print(\"clf_all_data:\")\n",
    "print(getsizeof(clf_all_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388358688\n",
      "573671\n"
     ]
    }
   ],
   "source": [
    "# print(getsizeof(clf_all_data1.centroids_[0][0]))\n",
    "aux = clf_all_data1.centroids_.data\n",
    "aux = aux.tolist()\n",
    "\n",
    "centroids = sp.csr_matrix(clf_all_data1.centroids_)\n",
    "classes = clf_all_data1.classes_\n",
    "\n",
    "n = 0\n",
    "s = 0\n",
    "for x in aux:\n",
    "    s += getsizeof(x)\n",
    "    n += 1\n",
    "print(s)\n",
    "\n",
    "print(clf_all_data1.classes_[70000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(centroids, open(\"centroids\", \"wb\"))\n",
    "pickle.dump(classes, open(\"classes\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pickle.load(open('centroids', 'rb'))\n",
    "classes = pickle.load(open('classes', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method spmatrix.asfptype of <79653x883149 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 42507558 stored elements in LInked List format>>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids.asfptype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pickle.load(open('vectorizer', 'rb'))\n",
    "\n",
    "data_vect = vectorizer.transform([\"Um método para deduplicação de metadados bibliográficos baseado no empilhamento de classificadores\"])\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "predict = np.argsort(pairwise_distances(data_vect, centroids, metric='euclidean'))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"select * from researcher;\"\n",
    "researcher = sqlio.read_sql_query(sql, conn)\n",
    "pickle.dump(researcher, open(\"researcher\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renata de matos galante\n",
      "george d. c. cavalcanti\n",
      "plácida leopoldina ventura amorim da costa santos\n",
      "lourenildo williame barbosa leite\n",
      "jorge luís machado do amaral\n",
      "george darmiton da cunha cavalcanti\n",
      "alberto henrique frade laender\n",
      "peter hubral\n",
      "cibele cecilio de faria rozenfeld\n",
      "carlos eduardo santos. pires\n",
      "jacob scharcanski\n",
      "anne magaly de paula canuto\n",
      "jose palazzo moreira de oliveira\n",
      "nina sumiko tomita hirata\n",
      "paulo jorge leitão adeodato\n"
     ]
    }
   ],
   "source": [
    "for i in classes[predict[0]][0:15]:\n",
    "    print(researcher['name'][researcher['id'] == i].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = pickle.load(open('vectorizer', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = pickle.load(open('centroids', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79653, 883149)\n",
      "  (0, 1)\t232\n",
      "  (0, 382)\t117\n",
      "  (0, 997)\t33\n",
      "  (0, 1094)\t190\n",
      "  (0, 1140)\t51\n",
      "  (0, 2046)\t33\n",
      "  (0, 3103)\t36\n",
      "  (0, 3637)\t29\n",
      "  (0, 4434)\t41\n",
      "  (0, 6323)\t123\n",
      "  (0, 7363)\t44\n",
      "  (0, 7546)\t38\n",
      "  (0, 8459)\t149\n",
      "  (0, 8758)\t39\n",
      "  (0, 9303)\t22\n",
      "  (0, 9540)\t49\n",
      "  (0, 9677)\t62\n",
      "  (0, 9701)\t54\n",
      "  (0, 10134)\t46\n",
      "  (0, 10662)\t40\n",
      "  (0, 11120)\t75\n",
      "  (0, 11890)\t49\n",
      "  (0, 12506)\t71\n",
      "  (0, 13171)\t101\n",
      "  (0, 13791)\t114\n",
      "  :\t:\n",
      "  (0, 854579)\t48\n",
      "  (0, 855112)\t23\n",
      "  (0, 856661)\t278\n",
      "  (0, 857854)\t38\n",
      "  (0, 857989)\t50\n",
      "  (0, 859497)\t129\n",
      "  (0, 859822)\t47\n",
      "  (0, 860118)\t129\n",
      "  (0, 860158)\t40\n",
      "  (0, 860161)\t36\n",
      "  (0, 867276)\t58\n",
      "  (0, 867300)\t67\n",
      "  (0, 876759)\t22\n",
      "  (0, 876775)\t25\n",
      "  (0, 876810)\t59\n",
      "  (0, 879038)\t77\n",
      "  (0, 879047)\t37\n",
      "  (0, 879378)\t397\n",
      "  (0, 879379)\t1306\n",
      "  (0, 879841)\t30\n",
      "  (0, 879845)\t24\n",
      "  (0, 879848)\t34\n",
      "  (0, 879876)\t28\n",
      "  (0, 879883)\t68\n",
      "  (0, 879917)\t29\n"
     ]
    }
   ],
   "source": [
    "print(centroids.shape)\n",
    "print((centroids[7]*100000).astype(np.uint16))\n",
    "centroids_min = (centroids*100000).astype(np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65940 é maior\n",
      "82086 é maior\n",
      "77984 é maior\n",
      "77984 é maior\n",
      "77984 é maior\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "row index (79653) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-eaf71e8167ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcentroid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#     print(centroid)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcentroid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m65535\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \"\"\"\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Dispatch to specialized methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mM\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'row index (%d) out of range'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mrow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: row index (79653) out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0,80000):\n",
    "    centroid = ((centroids[i]*100000).astype(np.uint32)).data\n",
    "#     print(centroid)\n",
    "    for j in centroid:\n",
    "        if j > 65535:\n",
    "            print(str(j)+\" é maior\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77984\n",
      "65535\n"
     ]
    }
   ],
   "source": [
    "x = np.int(77984)\n",
    "print(x)\n",
    "x = np.uint16(65535)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(centroids_min, open(\"centroids_min_uint16\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids_min.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "\n",
    "centroids_min = pickle.load(open('centroids_min_uint16', 'rb'))\n",
    "\n",
    "vectorizer = pickle.load(open('vectorizer', 'rb'))\n",
    "\n",
    "data_vect = vectorizer.transform([\"Um método para deduplicação de metadados bibliográficos baseado no empilhamento de classificadores\"])\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "predict = np.argsort(pairwise_distances(((data_vect*100000).astype(np.uint16)), centroids_min, metric='euclidean'))[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renata de matos galante\n",
      "george d. c. cavalcanti\n",
      "plácida leopoldina ventura amorim da costa santos\n",
      "lourenildo williame barbosa leite\n",
      "jorge luís machado do amaral\n",
      "george darmiton da cunha cavalcanti\n",
      "alberto henrique frade laender\n",
      "peter hubral\n",
      "cibele cecilio de faria rozenfeld\n",
      "carlos eduardo santos. pires\n",
      "jacob scharcanski\n",
      "anne magaly de paula canuto\n",
      "jose palazzo moreira de oliveira\n",
      "nina sumiko tomita hirata\n",
      "paulo jorge leitão adeodato\n"
     ]
    }
   ],
   "source": [
    "classes = pickle.load(open('classes', 'rb'))\n",
    "researcher = pickle.load(open('researcher', 'rb'))\n",
    "\n",
    "for i in classes[predict[0]][0:15]:\n",
    "    print(researcher['name'][researcher['id'] == i].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_min_csc = sp.csc_matrix(centroids_min)\n",
    "pickle.dump(centroids_min_csc, open(\"centroids_min_uint16_csc\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint16')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids_min_csc.dtype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
